{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTe3PbGxSgFy",
        "outputId": "0e870a8f-3152-45eb-d3de-994d709854ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XjIroWCMSest"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/TTS_2023_V3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cyR1Fyy8SVQf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import constants\n",
        "from dataset import Dataset\n",
        "from evaluate import evaluate\n",
        "from model import FastSpeech2Loss\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "#Instantly make your loops show a smart progress meter - just wrap any iterable with\n",
        "from tqdm import tqdm\n",
        "from model_utils import get_model, get_param_num\n",
        "from tools import log, synth_one_sample, to_device, get_vocoder\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train():\n",
        "    print(\"Getting ready for training ...\")\n",
        "\n",
        "    # preprocess_config, model_config, train_config = configs\n",
        "\n",
        "    # Get dataset\n",
        "    dataset = Dataset(\"train.txt\", sort=True, drop_last=True)\n",
        "    batch_size = constants.BATCH_SIZE\n",
        "    group_size = 4  # Set this larger than 1 to enable sorting in Dataset\n",
        "    assert batch_size * group_size < len(dataset)\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size * group_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=dataset.collate_fn,\n",
        "    )\n",
        "\n",
        "    # Prepare model\n",
        "    model, optimizer = get_model(constants.RESTORE_STEP, device, train=True)\n",
        "    model = nn.DataParallel(model)\n",
        "    num_param = get_param_num(model)\n",
        "    Loss = FastSpeech2Loss().to(device)\n",
        "    print(\"Number of FastSpeech2 Parameters:\", num_param)\n",
        "\n",
        "    # Load vocoder\n",
        "    vocoder = get_vocoder(device, constants.VOCODER_CONFIG_PATH, constants.VOCODER_PRETRAINED_MODEL_PATH)\n",
        "\n",
        "    os.makedirs(constants.CKPT_PATH, exist_ok=True)\n",
        "    os.makedirs(constants.LOG_PATH, exist_ok=True)\n",
        "    os.makedirs(constants.RESULT_PATH, exist_ok=True)\n",
        "\n",
        "    train_log_path = constants.LOG_PATH + \"/train\"\n",
        "    val_log_path = constants.LOG_PATH + \"/val\"\n",
        "    os.makedirs(train_log_path, exist_ok=True)\n",
        "    os.makedirs(val_log_path, exist_ok=True)\n",
        "    train_logger = SummaryWriter(train_log_path)\n",
        "    val_logger = SummaryWriter(val_log_path)\n",
        "\n",
        "    # Training\n",
        "    step =  constants.RESTORE_STEP+1\n",
        "    epoch = 1\n",
        "    grad_acc_step = constants.GRAD_ACC_STEP\n",
        "    grad_clip_thresh = constants.GRAD_CLIP_THRESH\n",
        "    total_step = constants.TOTAL_STEP\n",
        "    log_step = constants.LOG_STEP\n",
        "    save_step = constants.SAVE_STEP\n",
        "    synth_step = constants.SYNTH_STEP\n",
        "    val_step = constants.VAL_STEP\n",
        "    print('total_step:', total_step)\n",
        "    print('restore_step:',constants.RESTORE_STEP)\n",
        "    print('grad_acc_step:',grad_acc_step)\n",
        "    print('grad_clip_thresh:',grad_clip_thresh)\n",
        "    print('log_step:',log_step)\n",
        "    print('save_step:',save_step)\n",
        "    print('synth_step:',synth_step)\n",
        "    print('val_step:',val_step)\n",
        "    outer_bar = tqdm(total=total_step, desc=\"Training\", position=0)\n",
        "    outer_bar.n = constants.RESTORE_STEP\n",
        "    outer_bar.update()\n",
        "    while True:\n",
        "        # inner_bar = tqdm(total=len(loader), desc=\"Epoch {}\".format(epoch), position=1)\n",
        "        for batchs in loader:\n",
        "            for batch in batchs:\n",
        "\n",
        "                batch = to_device(batch, device)\n",
        "\n",
        "                # Forward\n",
        "                output = model(*(batch[2:]))\n",
        "\n",
        "                # Cal Loss\n",
        "                losses = Loss(batch, output)\n",
        "                total_loss = losses[0]\n",
        "\n",
        "                # Backward\n",
        "                total_loss = total_loss / grad_acc_step\n",
        "                total_loss.backward()\n",
        "                if step % grad_acc_step == 0:\n",
        "                    # Clipping gradients to avoid gradient explosion\n",
        "                    nn.utils.clip_grad_norm_(model.parameters(), grad_clip_thresh)\n",
        "\n",
        "                    # Update weights\n",
        "                    optimizer.step_and_update_lr()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                if step % log_step == 0:\n",
        "                    losses = [l.item() for l in losses]\n",
        "                    message1 = \"Step {}/{}, \".format(step, total_step)\n",
        "                    message2 = \"Total Loss: {:.4f}, Mel Loss: {:.4f}, Mel PostNet Loss: {:.4f}, Pitch Loss: {:.4f}, Energy Loss: {:.4f}, Duration Loss: {:.4f}\".format(\n",
        "                        *losses\n",
        "                    )\n",
        "\n",
        "                    with open(os.path.join(train_log_path, \"log.txt\"), \"a\") as f:\n",
        "                        f.write(message1 + message2 + \"\\n\")\n",
        "\n",
        "                    outer_bar.write(message1 + message2)\n",
        "\n",
        "                    log(train_logger, step, losses=losses)\n",
        "\n",
        "                if step % synth_step == 0:\n",
        "                    fig, wav_reconstruction, wav_prediction, tag = synth_one_sample(\n",
        "                        batch,\n",
        "                        output,\n",
        "                        vocoder,\n",
        "                        constants\n",
        "                    )\n",
        "                    log(\n",
        "                        train_logger,\n",
        "                        fig=fig,\n",
        "                        tag=\"Training/step_{}_{}\".format(step, tag),\n",
        "                    )\n",
        "                    sampling_rate = constants.SAMPLING_RATE\n",
        "                    log(\n",
        "                        train_logger,\n",
        "                        audio=wav_reconstruction,\n",
        "                        sampling_rate=sampling_rate,\n",
        "                        tag=\"Training/step_{}_{}_reconstructed\".format(step, tag),\n",
        "                    )\n",
        "                    log(\n",
        "                        train_logger,\n",
        "                        audio=wav_prediction,\n",
        "                        sampling_rate=sampling_rate,\n",
        "                        tag=\"Training/step_{}_{}_synthesized\".format(step, tag),\n",
        "                    )\n",
        "\n",
        "                if step % val_step == 0:\n",
        "                    model.eval()\n",
        "                    message = evaluate(model, step, val_logger, vocoder)\n",
        "                    with open(os.path.join(val_log_path, \"log.txt\"), \"a\") as f:\n",
        "                        f.write(message + \"\\n\")\n",
        "                    outer_bar.write(message)\n",
        "\n",
        "                    model.train()\n",
        "\n",
        "                if step % save_step == 0:\n",
        "                    torch.save(\n",
        "                        {\n",
        "                            \"model\": model.module.state_dict(),\n",
        "                            \"optimizer\": optimizer._optimizer.state_dict(),\n",
        "                        },\n",
        "                        os.path.join(\n",
        "                            constants.CKPT_PATH,\n",
        "                            \"{}.pth.tar\".format(step),\n",
        "                        ),\n",
        "                    )\n",
        "                if step>=total_step:\n",
        "                    break\n",
        "\n",
        "                step += 1\n",
        "                outer_bar.update(1)\n",
        "\n",
        "            if step>=total_step:\n",
        "                    break\n",
        "\n",
        "        if step>=total_step:\n",
        "                    break\n",
        "        epoch += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6AJrN0ESVQp",
        "outputId": "9170b1b7-5be2-4be4-c5f6-6a0421c6aeb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting ready for training ...\n",
            "Number of FastSpeech2 Parameters: 35186497\n",
            "Removing weight norm...\n",
            "total_step: 1045000\n",
            "restore_step: 1040000\n",
            "grad_acc_step: 1\n",
            "grad_clip_thresh: 1.0\n",
            "log_step: 1000\n",
            "save_step: 5000\n",
            "synth_step: 1000\n",
            "val_step: 10000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|█████████▉| 1041000/1045000 [17:21<54:10,  1.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1041000/1045000, Total Loss: 0.5602, Mel Loss: 0.2561, Mel PostNet Loss: 0.2555, Pitch Loss: 0.0187, Energy Loss: 0.0162, Duration Loss: 0.0137\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|█████████▉| 1042000/1045000 [31:00<03:17, 15.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1042000/1045000, Total Loss: 0.7109, Mel Loss: 0.3259, Mel PostNet Loss: 0.3253, Pitch Loss: 0.0261, Energy Loss: 0.0181, Duration Loss: 0.0156\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|█████████▉| 1043000/1045000 [32:13<02:18, 14.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1043000/1045000, Total Loss: 0.7298, Mel Loss: 0.3271, Mel PostNet Loss: 0.3263, Pitch Loss: 0.0287, Energy Loss: 0.0223, Duration Loss: 0.0254\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|█████████▉| 1044000/1045000 [33:26<01:17, 12.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1044000/1045000, Total Loss: 0.6110, Mel Loss: 0.2685, Mel PostNet Loss: 0.2676, Pitch Loss: 0.0326, Energy Loss: 0.0185, Duration Loss: 0.0237\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 1045000/1045000 [34:39<00:00, 13.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1045000/1045000, Total Loss: 0.7228, Mel Loss: 0.3209, Mel PostNet Loss: 0.3201, Pitch Loss: 0.0241, Energy Loss: 0.0304, Duration Loss: 0.0273\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining: 100%|██████████| 1045000/1045000 [34:41<00:00, 502.00it/s]\n"
          ]
        }
      ],
      "source": [
        "train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
